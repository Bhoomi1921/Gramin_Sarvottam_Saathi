{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf66531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in d:\\face_recog\\env\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\face_recog\\env\\lib\\site-packages (1.7.2)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "     ------------------------------------ 294.9/294.9 kB 173.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib in d:\\face_recog\\env\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: keras>=3.10.0 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (25.12.19)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: setuptools in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: packaging in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in d:\\face_recog\\env\\lib\\site-packages (from tensorflow) (6.33.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\face_recog\\env\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\face_recog\\env\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\face_recog\\env\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: pandas>=1.2 in d:\\face_recog\\env\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: pyparsing>=3 in d:\\face_recog\\env\\lib\\site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: pillow>=8 in d:\\face_recog\\env\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\face_recog\\env\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\face_recog\\env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\face_recog\\env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\face_recog\\env\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\face_recog\\env\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\face_recog\\env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: namex in d:\\face_recog\\env\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: rich in d:\\face_recog\\env\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: optree in d:\\face_recog\\env\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\face_recog\\env\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\face_recog\\env\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\face_recog\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\face_recog\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\face_recog\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\face_recog\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\face_recog\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\face_recog\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\face_recog\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.4)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in d:\\face_recog\\env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\face_recog\\env\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\face_recog\\env\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\face_recog\\env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow scikit-learn seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8175d866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "     ------------------------------------ 250.9/250.9 kB 149.6 kB/s eta 0:00:00\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (d:\\face_recog\\env\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283bc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total found: 26608\n",
      "Train: 1400, Val: 400, Test: 200\n",
      "Error: 609_0.24478180162880647 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci82MDlfMC4yNDQ3ODE4MDE2Mjg4MDY0Nw~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16123280>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 583_0.8620113000082098 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci81ODNfMC44NjIwMTEzMDAwMDgyMDk4 (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16123940>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 549_0.6114094757927416 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci81NDlfMC42MTE0MDk0NzU3OTI3NDE2 (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16123eb0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 564_0.6954938939122288 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci81NjRfMC42OTU0OTM4OTM5MTIyMjg4 (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16144cd0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1246_0.22826758197228747 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xMjQ2XzAuMjI4MjY3NTgxOTcyMjg3NDc~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16145330>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 594_0.6081324353376072 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci81OTRfMC42MDgxMzI0MzUzMzc2MDcy (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16144fd0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1975_0.33499334387698254 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xOTc1XzAuMzM0OTkzMzQzODc2OTgyNTQ~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16144a00>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 803_0.5403698315181206 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci84MDNfMC41NDAzNjk4MzE1MTgxMjA2 (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16145840>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1103_0.5963098334702281 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xMTAzXzAuNTk2MzA5ODMzNDcwMjI4MQ~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16145db0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1677_0.9413578106538408 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xNjc3XzAuOTQxMzU3ODEwNjUzODQwOA~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16146320>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1399_0.9620427131957862 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xMzk5XzAuOTYyMDQyNzEzMTk1Nzg2Mg~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b161477c0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1564_0.0676830180089183 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xNTY0XzAuMDY3NjgzMDE4MDA4OTE4Mw~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16178340>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 973_0.343656945757723 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci85NzNfMC4zNDM2NTY5NDU3NTc3MjM~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16145810>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))Error: 813_0.16198382824796187 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci84MTNfMC4xNjE5ODM4MjgyNDc5NjE4Nw~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16178f70>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 522_0.4713275473773374 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci81MjJfMC40NzEzMjc1NDczNzczMzc0 (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16178ca0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1448_0.4938913685352889 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xNDQ4XzAuNDkzODkxMzY4NTM1Mjg4OQ~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b154524a0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 154_0.9120889250683322 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xNTRfMC45MTIwODg5MjUwNjgzMzIy (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b166c4eb0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "\n",
      "Error: 675_0.20568541000310714 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci82NzVfMC4yMDU2ODU0MTAwMDMxMDcxNA~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16179c00>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 155_0.5068302134299334 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xNTVfMC41MDY4MzAyMTM0Mjk5MzM0 (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b1617a0e0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1799_0.6855262336274017 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xNzk5XzAuNjg1NTI2MjMzNjI3NDAxNw~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b1617aaa0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 348_0.9354838714443134 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8zNDhfMC45MzU0ODM4NzE0NDQzMTM0 (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b15e73640>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 755_0.01633441916158762 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci83NTVfMC4wMTYzMzQ0MTkxNjE1ODc2Mg~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b166c4700>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1889_0.04395661075442192 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xODg5XzAuMDQzOTU2NjEwNzU0NDIxOTI~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b161472b0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 2013_0.9009962205036037 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8yMDEzXzAuOTAwOTk2MjIwNTAzNjAzNw~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b15e72770>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 496_0.7944328326261405 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci80OTZfMC43OTQ0MzI4MzI2MjYxNDA1 (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16147970>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1622_0.8239540078906245 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xNjIyXzAuODIzOTU0MDA3ODkwNjI0NQ~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16145840>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1468_0.200770552168959 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xNDY4XzAuMjAwNzcwNTUyMTY4OTU5 (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16146680>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 24_0.7589720621699135 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8yNF8wLjc1ODk3MjA2MjE2OTkxMzU~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b161445e0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1865_0.6763045848073111 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xODY1XzAuNjc2MzA0NTg0ODA3MzExMQ~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16145ba0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 286_0.9050682725280358 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8yODZfMC45MDUwNjgyNzI1MjgwMzU4 (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16144ee0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 584_0.19315690868251023 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci81ODRfMC4xOTMxNTY5MDg2ODI1MTAyMw~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b16123850>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1323_0.40342444620633844 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xMzIzXzAuNDAzNDI0NDQ2MjA2MzM4NDQ~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b15e71360>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1094_0.4738343198104996 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xMDk0XzAuNDczODM0MzE5ODEwNDk5Ng~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b15e70d60>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 459_0.5000546176669655 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci80NTlfMC41MDAwNTQ2MTc2NjY5NjU1 (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b161070a0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n",
      "Error: 1921_0.5615707858095595 -> HTTPConnectionPool(host='192.168.1.51', port=8090): Max retries exceeded with url: /ips/block/webcat?cat=83&pl=0&lu=0&url=aHR0cDovLzUyLjE2OC4xNzkuMTAxL2FwZ19pbWFnZS9oZWFkcXVhcnRlci8xOTIxXzAuNTYxNTcwNzg1ODA5NTU5NQ~~ (Caused by ConnectTimeoutError(<HTTPConnection(host='192.168.1.51', port=8090) at 0x24b161060b0>, 'Connection to 192.168.1.51 timed out. (connect timeout=10)'))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "EXCEL_FILE = 'sheet.xlsx'\n",
    "BASE_URL = 'http://52.168.179.101/apg_image/headquarter/'\n",
    "DATASET_DIR = 'dataset'\n",
    "\n",
    "TOTAL_IMAGES = 2000\n",
    "MAX_WORKERS = 10\n",
    "\n",
    "# Split ratios\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.2\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "# Create dataset folders\n",
    "train_dir = os.path.join(DATASET_DIR, \"train\")\n",
    "val_dir = os.path.join(DATASET_DIR, \"val\")\n",
    "test_dir = os.path.join(DATASET_DIR, \"test\")\n",
    "\n",
    "for d in [train_dir, val_dir, test_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "\n",
    "def download_image(args):\n",
    "    image_name, output_dir = args\n",
    "\n",
    "    try:\n",
    "        url = f\"{BASE_URL}{image_name}\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            file_prefix = image_name.split('_')[0] if '_' in image_name else image_name\n",
    "            file_path = os.path.join(output_dir, f\"{file_prefix}.png\")\n",
    "\n",
    "            with open(file_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "            return True, image_name\n",
    "        else:\n",
    "            print(f\"Failed: {image_name} ({response.status_code})\")\n",
    "            return False, image_name\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {image_name} -> {e}\")\n",
    "        return False, image_name\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        df = pd.read_excel(EXCEL_FILE, sheet_name='Sheet1')\n",
    "    except Exception as e:\n",
    "        print(f\"Excel error: {e}\")\n",
    "        return\n",
    "\n",
    "    image_names = df['Image Name'].dropna().unique().tolist()\n",
    "\n",
    "    if not image_names:\n",
    "        print(\"No image names found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total found: {len(image_names)}\")\n",
    "\n",
    "    # Limit to 2000\n",
    "    image_names = image_names[:TOTAL_IMAGES]\n",
    "\n",
    "    # Shuffle for random split\n",
    "    random.shuffle(image_names)\n",
    "\n",
    "    # Compute splits\n",
    "    total = len(image_names)\n",
    "    train_end = int(total * TRAIN_RATIO)\n",
    "    val_end = train_end + int(total * VAL_RATIO)\n",
    "\n",
    "    train_images = image_names[:train_end]\n",
    "    val_images = image_names[train_end:val_end]\n",
    "    test_images = image_names[val_end:]\n",
    "\n",
    "    print(f\"Train: {len(train_images)}, Val: {len(val_images)}, Test: {len(test_images)}\")\n",
    "\n",
    "    # Prepare download tasks\n",
    "    tasks = (\n",
    "        [(img, train_dir) for img in train_images] +\n",
    "        [(img, val_dir) for img in val_images] +\n",
    "        [(img, test_dir) for img in test_images]\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        results = list(executor.map(download_image, tasks))\n",
    "\n",
    "    success_count = sum(1 for r in results if r[0])\n",
    "\n",
    "    print(\"\\nDownload completed!\")\n",
    "    print(f\"Downloaded {success_count}/{len(tasks)}\")\n",
    "    print(f\"Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    failed = [name for success, name in results if not success]\n",
    "    if failed:\n",
    "        with open(\"failed_downloads.txt\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(failed))\n",
    "        print(\"Failed list saved.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ea27d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: 0\n",
      "\n",
      "ğŸ“‚ Loading data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'dataset/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 404\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ensemble, class_names\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 404\u001b[0m     ensemble_model, class_names \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 346\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# â”€â”€ Data â”€â”€\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ“‚ Loading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 346\u001b[0m train_gen, val_gen, test_gen \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(train_gen\u001b[38;5;241m.\u001b[39mclass_indices\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Train samples : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_gen\u001b[38;5;241m.\u001b[39msamples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 70\u001b[0m, in \u001b[0;36mget_data_generators\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m     57\u001b[0m     rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m,\n\u001b[0;32m     58\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     fill_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m )\n\u001b[0;32m     68\u001b[0m val_test_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m---> 70\u001b[0m train_gen \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m val_gen \u001b[38;5;241m=\u001b[39m val_test_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     75\u001b[0m     VAL_DIR, target_size\u001b[38;5;241m=\u001b[39mIMAGE_SIZE,\n\u001b[0;32m     76\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m\"\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     77\u001b[0m )\n\u001b[0;32m     78\u001b[0m test_gen \u001b[38;5;241m=\u001b[39m val_test_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     79\u001b[0m     TEST_DIR, target_size\u001b[38;5;241m=\u001b[39mIMAGE_SIZE,\n\u001b[0;32m     80\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m\"\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     81\u001b[0m )\n",
      "File \u001b[1;32md:\\Face_Recog\\env\\lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1136\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[1;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1120\u001b[0m     directory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1134\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1135\u001b[0m ):\n\u001b[1;32m-> 1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Face_Recog\\env\\lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:456\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[1;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m    455\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    458\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'dataset/train'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================\n",
    "  FACE RECOGNITION USING ENSEMBLE + TRANSFER LEARNING\n",
    "  Framework: TensorFlow / Keras\n",
    "============================================================\n",
    "  Techniques Used:\n",
    "  - Transfer Learning: VGG16, ResNet50, MobileNetV2 (pretrained on ImageNet)\n",
    "  - Ensemble Learning: Soft Voting (average of probabilities)\n",
    "  - Fine-tuning: Unfreeze top layers of each base model\n",
    "  - Data Augmentation for robustness\n",
    "============================================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  1. CONFIGURATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IMAGE_SIZE    = (160, 160)     # Input image size (H x W)\n",
    "BATCH_SIZE    = 32\n",
    "EPOCHS_FROZEN = 10             # Epochs while base model is frozen\n",
    "EPOCHS_FINETUNE = 10           # Epochs after unfreezing top layers\n",
    "LEARNING_RATE = 1e-4\n",
    "FINETUNE_LR   = 1e-5           # Lower LR for fine-tuning\n",
    "NUM_CLASSES   = 5              # â† Change to your number of face classes\n",
    "\n",
    "# Dataset paths (update these to your directory structure)\n",
    "# Structure expected:\n",
    "#   dataset/\n",
    "#     train/  class_A/  class_B/ ...\n",
    "#     val/    class_A/  class_B/ ...\n",
    "#     test/   class_A/  class_B/ ...\n",
    "TRAIN_DIR = \"dataset/train\"\n",
    "VAL_DIR   = \"dataset/val\"\n",
    "TEST_DIR  = \"dataset/test\"\n",
    "\n",
    "MODELS_SAVE_DIR = \"saved_models\"\n",
    "os.makedirs(MODELS_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  2. DATA AUGMENTATION & LOADING\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_data_generators():\n",
    "    \"\"\"Create augmented data generators for train/val/test.\"\"\"\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0 / 255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.15,\n",
    "        height_shift_range=0.15,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.15,\n",
    "        horizontal_flip=True,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        fill_mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    val_test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR, target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE, class_mode=\"categorical\", shuffle=True\n",
    "    )\n",
    "    val_gen = val_test_datagen.flow_from_directory(\n",
    "        VAL_DIR, target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE, class_mode=\"categorical\", shuffle=False\n",
    "    )\n",
    "    test_gen = val_test_datagen.flow_from_directory(\n",
    "        TEST_DIR, target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE, class_mode=\"categorical\", shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_gen, val_gen, test_gen\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  3. TRANSFER LEARNING BASE MODELS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_transfer_model(base_arch: str, num_classes: int, input_shape=(160, 160, 3)):\n",
    "    \"\"\"\n",
    "    Builds a transfer learning model with a custom classification head.\n",
    "\n",
    "    Args:\n",
    "        base_arch: One of 'vgg16', 'resnet50', 'mobilenetv2'\n",
    "        num_classes: Number of face identity classes\n",
    "        input_shape: Input image shape (H, W, C)\n",
    "\n",
    "    Returns:\n",
    "        Keras Model\n",
    "    \"\"\"\n",
    "\n",
    "    # â”€â”€ Load pre-trained base (ImageNet weights, exclude top layers) â”€â”€\n",
    "    arch_map = {\n",
    "        \"vgg16\":       VGG16,\n",
    "        \"resnet50\":    ResNet50,\n",
    "        \"mobilenetv2\": MobileNetV2,\n",
    "    }\n",
    "    assert base_arch in arch_map, f\"Unknown architecture: {base_arch}\"\n",
    "\n",
    "    base_model = arch_map[base_arch](\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Freeze all base layers initially â”€â”€\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # â”€â”€ Build custom head â”€â”€\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=f\"{base_arch}_input\")\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D(name=f\"{base_arch}_gap\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\", name=f\"{base_arch}_fc1\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(256, activation=\"relu\", name=f\"{base_arch}_fc2\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=f\"{base_arch}_output\")(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs, name=f\"FaceRec_{base_arch.upper()}\")\n",
    "    return model, base_model\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  4. FINE-TUNING UTILITY\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def unfreeze_top_layers(base_model, num_layers_to_unfreeze: int):\n",
    "    \"\"\"\n",
    "    Unfreezes the last N layers of the base model for fine-tuning.\n",
    "    All earlier layers remain frozen (preserve low-level ImageNet features).\n",
    "    \"\"\"\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-num_layers_to_unfreeze]:\n",
    "        layer.trainable = False\n",
    "    print(f\"  â†’ Unfroze last {num_layers_to_unfreeze} layers of {base_model.name}\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  5. TRAINING A SINGLE MODEL (PHASE 1 + 2)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def train_model(arch_name: str, train_gen, val_gen, num_classes: int):\n",
    "    \"\"\"\n",
    "    Full two-phase training:\n",
    "      Phase 1 - Train custom head only (base frozen)\n",
    "      Phase 2 - Fine-tune top layers of base model\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*55}\")\n",
    "    print(f\"  Training Model: {arch_name.upper()}\")\n",
    "    print(f\"{'='*55}\")\n",
    "\n",
    "    model, base_model = build_transfer_model(arch_name, num_classes)\n",
    "    model.summary()\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=5, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
    "        ReduceLROnPlateau(patience=3, factor=0.5, min_lr=1e-7, verbose=1),\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(MODELS_SAVE_DIR, f\"best_{arch_name}.keras\"),\n",
    "            save_best_only=True, monitor=\"val_accuracy\", verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # â”€â”€ Phase 1: Train head only â”€â”€\n",
    "    print(f\"\\n[Phase 1] Training classification head â€” base model frozen\")\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(LEARNING_RATE),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    history1 = model.fit(\n",
    "        train_gen, validation_data=val_gen,\n",
    "        epochs=EPOCHS_FROZEN, callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Phase 2: Fine-tune top layers â”€â”€\n",
    "    print(f\"\\n[Phase 2] Fine-tuning top layers of {arch_name.upper()}\")\n",
    "    unfreeze_layers = {\"vgg16\": 4, \"resnet50\": 15, \"mobilenetv2\": 20}\n",
    "    unfreeze_top_layers(base_model, unfreeze_layers.get(arch_name, 10))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(FINETUNE_LR),   # Lower LR is critical!\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    history2 = model.fit(\n",
    "        train_gen, validation_data=val_gen,\n",
    "        epochs=EPOCHS_FINETUNE, callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Load best checkpoint\n",
    "    model.load_weights(os.path.join(MODELS_SAVE_DIR, f\"best_{arch_name}.keras\"))\n",
    "    print(f\"\\nâœ” {arch_name.upper()} training complete.\\n\")\n",
    "\n",
    "    return model, history1, history2\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  6. ENSEMBLE MODEL (SOFT VOTING)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_ensemble(models_list: list, input_shape=(160, 160, 3)):\n",
    "    \"\"\"\n",
    "    Combines multiple models using Soft Voting:\n",
    "    Final prediction = average of all models' output probabilities.\n",
    "    Each model gets equal weight (can be customized).\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=\"ensemble_input\")\n",
    "\n",
    "    # Collect softmax outputs from each model\n",
    "    outputs = [model(inputs) for model in models_list]\n",
    "\n",
    "    # Average the probabilities (soft voting)\n",
    "    if len(outputs) > 1:\n",
    "        ensemble_output = layers.Average(name=\"soft_vote_avg\")(outputs)\n",
    "    else:\n",
    "        ensemble_output = outputs[0]\n",
    "\n",
    "    ensemble_model = models.Model(\n",
    "        inputs=inputs,\n",
    "        outputs=ensemble_output,\n",
    "        name=\"FaceRec_Ensemble\"\n",
    "    )\n",
    "    return ensemble_model\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  7. EVALUATION UTILITIES\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate_model(model, test_gen, model_name=\"Model\"):\n",
    "    \"\"\"Full evaluation: accuracy, classification report, confusion matrix.\"\"\"\n",
    "    print(f\"\\nğŸ“Š Evaluating: {model_name}\")\n",
    "    loss, acc = model.evaluate(test_gen, verbose=0)\n",
    "    print(f\"   Test Loss     : {loss:.4f}\")\n",
    "    print(f\"   Test Accuracy : {acc*100:.2f}%\")\n",
    "\n",
    "    # Predictions\n",
    "    test_gen.reset()\n",
    "    y_pred_probs = model.predict(test_gen, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = test_gen.classes\n",
    "    class_names = list(test_gen.class_indices.keys())\n",
    "\n",
    "    print(\"\\nğŸ“‹ Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    return y_true, y_pred, class_names, acc\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=class_names, yticklabels=class_names\n",
    "    )\n",
    "    plt.title(title, fontsize=14, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{title.replace(' ', '_')}.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"  Saved: {title.replace(' ', '_')}.png\")\n",
    "\n",
    "\n",
    "def plot_training_history(history1, history2, arch_name):\n",
    "    \"\"\"Plot combined training accuracy/loss across both phases.\"\"\"\n",
    "    acc  = history1.history[\"accuracy\"]  + history2.history[\"accuracy\"]\n",
    "    val_acc = history1.history[\"val_accuracy\"] + history2.history[\"val_accuracy\"]\n",
    "    loss = history1.history[\"loss\"]      + history2.history[\"loss\"]\n",
    "    val_loss = history1.history[\"val_loss\"]    + history2.history[\"val_loss\"]\n",
    "    split = len(history1.history[\"accuracy\"])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle(f\"{arch_name.upper()} â€” Training History\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    ax1.plot(acc, label=\"Train Acc\")\n",
    "    ax1.plot(val_acc, label=\"Val Acc\")\n",
    "    ax1.axvline(split, color=\"gray\", linestyle=\"--\", label=\"Fine-tuning starts\")\n",
    "    ax1.set_title(\"Accuracy\"); ax1.legend(); ax1.set_xlabel(\"Epoch\")\n",
    "\n",
    "    ax2.plot(loss, label=\"Train Loss\")\n",
    "    ax2.plot(val_loss, label=\"Val Loss\")\n",
    "    ax2.axvline(split, color=\"gray\", linestyle=\"--\", label=\"Fine-tuning starts\")\n",
    "    ax2.set_title(\"Loss\"); ax2.legend(); ax2.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"history_{arch_name}.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  8. PREDICT A SINGLE IMAGE\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def predict_face(ensemble_model, image_path: str, class_names: list):\n",
    "    \"\"\"\n",
    "    Predict the identity of a single face image.\n",
    "\n",
    "    Args:\n",
    "        ensemble_model: Trained ensemble model\n",
    "        image_path: Path to input face image\n",
    "        class_names: List of class labels\n",
    "\n",
    "    Returns:\n",
    "        predicted_class (str), confidence (float)\n",
    "    \"\"\"\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=IMAGE_SIZE)\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)   # Shape: (1, H, W, 3)\n",
    "\n",
    "    preds = ensemble_model.predict(img_array, verbose=0)[0]\n",
    "    pred_idx = np.argmax(preds)\n",
    "    confidence = preds[pred_idx]\n",
    "\n",
    "    print(f\"\\nğŸ” Prediction: {class_names[pred_idx]}  (confidence: {confidence*100:.1f}%)\")\n",
    "\n",
    "    # Show top-3 predictions\n",
    "    top3_idx = np.argsort(preds)[::-1][:3]\n",
    "    print(\"   Top-3 predictions:\")\n",
    "    for i, idx in enumerate(top3_idx):\n",
    "        print(f\"     {i+1}. {class_names[idx]}: {preds[idx]*100:.1f}%\")\n",
    "\n",
    "    return class_names[pred_idx], float(confidence)\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  9. MAIN PIPELINE\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main():\n",
    "    # â”€â”€ GPU check â”€â”€\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    print(f\"GPUs available: {len(gpus)}\")\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "    # â”€â”€ Data â”€â”€\n",
    "    print(\"\\nğŸ“‚ Loading data...\")\n",
    "    train_gen, val_gen, test_gen = get_data_generators()\n",
    "    print(f\"   Classes: {list(train_gen.class_indices.keys())}\")\n",
    "    print(f\"   Train samples : {train_gen.samples}\")\n",
    "    print(f\"   Val samples   : {val_gen.samples}\")\n",
    "    print(f\"   Test samples  : {test_gen.samples}\")\n",
    "\n",
    "    # â”€â”€ Train individual models â”€â”€\n",
    "    architectures = [\"mobilenetv2\", \"resnet50\", \"vgg16\"]\n",
    "    trained_models   = []\n",
    "    all_histories    = []\n",
    "\n",
    "    for arch in architectures:\n",
    "        model, h1, h2 = train_model(arch, train_gen, val_gen, NUM_CLASSES)\n",
    "        trained_models.append(model)\n",
    "        all_histories.append((arch, h1, h2))\n",
    "        plot_training_history(h1, h2, arch)\n",
    "\n",
    "    # â”€â”€ Build ensemble â”€â”€\n",
    "    print(\"\\nğŸ”— Building Ensemble Model (Soft Voting)...\")\n",
    "    ensemble = build_ensemble(trained_models, input_shape=(*IMAGE_SIZE, 3))\n",
    "    ensemble.summary()\n",
    "\n",
    "    # â”€â”€ Evaluate each model individually â”€â”€\n",
    "    results = {}\n",
    "    for arch, model in zip(architectures, trained_models):\n",
    "        y_true, y_pred, class_names, acc = evaluate_model(\n",
    "            model, test_gen, model_name=arch.upper()\n",
    "        )\n",
    "        results[arch] = acc\n",
    "        plot_confusion_matrix(y_true, y_pred, class_names, title=f\"CM_{arch.upper()}\")\n",
    "\n",
    "    # â”€â”€ Evaluate ensemble â”€â”€\n",
    "    y_true, y_pred, class_names, acc_ensemble = evaluate_model(\n",
    "        ensemble, test_gen, model_name=\"ENSEMBLE\"\n",
    "    )\n",
    "    results[\"ensemble\"] = acc_ensemble\n",
    "    plot_confusion_matrix(y_true, y_pred, class_names, title=\"CM_ENSEMBLE\")\n",
    "\n",
    "    # â”€â”€ Summary table â”€â”€\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"  FINAL ACCURACY COMPARISON\")\n",
    "    print(\"=\"*40)\n",
    "    for name, acc in results.items():\n",
    "        marker = \" â† BEST\" if acc == max(results.values()) else \"\"\n",
    "        print(f\"  {name.upper():<15}: {acc*100:.2f}%{marker}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # â”€â”€ Save ensemble â”€â”€\n",
    "    ensemble.save(os.path.join(MODELS_SAVE_DIR, \"face_recognition_ensemble.keras\"))\n",
    "    print(f\"\\nâœ… Ensemble model saved to: {MODELS_SAVE_DIR}/face_recognition_ensemble.keras\")\n",
    "\n",
    "    # â”€â”€ Example single-image prediction â”€â”€\n",
    "    # predict_face(ensemble, \"test_face.jpg\", class_names)\n",
    "\n",
    "    return ensemble, class_names\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ensemble_model, class_names = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8526a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
